{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vzCoWYtFUUcD"
      },
      "outputs": [],
      "source": [
        "# Part 1\n",
        "# You will essentially build your classifier as follows:\n",
        "# 1. Import libraries, modules, and packages you will need. Make sure to import the *preprocess_input* function from <code>keras.applications.vgg16</code>.\n",
        "# 2. Use a batch size of 100 images for both training and validation.\n",
        "# 3. Construct an ImageDataGenerator for the training set and another one for the validation set. VGG16 was originally trained on 224 × 224 images, so make sure to address that when defining the ImageDataGenerator instances.\n",
        "# 4. Create a sequential model using Keras. Add VGG16 model to it and dense layer.\n",
        "# 5. Compile the mode using the adam optimizer and the categorical_crossentropy loss function.\n",
        "# 6. Fit the model on the augmented data using the ImageDataGenerators."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps to Build the Image Classifier using VGG16:\n",
        "# First, import the required libraries, including the VGG16 model, preprocess_input, and others for handling data and defining the model"
      ],
      "metadata": {
        "id": "qHHVoc9UUhL8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.applications import VGG16, ResNet50\n",
        "from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg\n",
        "from keras.applications.resnet50 import preprocess_input as preprocess_input_resnet\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "Hdy1JkgjUjvm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the data from unzip file for google colabs\n",
        "# Step 1: Download the zip file using wget\n",
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip\n",
        "\n",
        "# Step 2: Unzip the file\n",
        "!unzip concrete_data_week4.zip\n",
        "\n",
        "# Step 3: Verify the contents of the unzipped folder\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWmimKmxUkcb",
        "outputId": "7545a9c6-0530-4476-a9ff-e7ff0e945ee2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-01 09:11:54--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 261483817 (249M) [application/zip]\n",
            "Saving to: ‘concrete_data_week4.zip.1’\n",
            "\n",
            "a_week4.zip.1        64%[===========>        ] 160.01M  29.8MB/s    eta 3s     ^C\n",
            "Archive:  concrete_data_week4.zip\n",
            "replace concrete_data_week4/valid/positive/16679_1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: concrete_data_week4  concrete_data_week4.zip  concrete_data_week4.zip.1  __MACOSX  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Prepare the Data Generators\n",
        "# VGG16 was trained on images of size 224x224. We will use this target size for both models."
      ],
      "metadata": {
        "id": "Vgtod4VQUr2v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ImageDataGenerators for training and validation\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg)\n",
        "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg)\n",
        "\n",
        "# Path to your dataset directories\n",
        "train_dir = '/content/concrete_data_week4/train'\n",
        "valid_dir = '/content/concrete_data_week4/valid'\n",
        "\n",
        "# Load the training and validation data\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    target_size=(224, 224),\n",
        "                                                    batch_size=100,\n",
        "                                                    class_mode='categorical')\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(valid_dir,\n",
        "                                                    target_size=(224, 224),\n",
        "                                                    batch_size=100,\n",
        "                                                    class_mode='categorical')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWsYUaWFVB5h",
        "outputId": "616d4333-23be-4a4e-8b0f-e0fa04d349a8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 30001 images belonging to 2 classes.\n",
            "Found 9501 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Build the VGG16-based Classifier\n",
        "# Load VGG16 without the top layers\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Build a Sequential model\n",
        "vgg16_model = Sequential()\n",
        "\n",
        "# Add VGG16 base model\n",
        "vgg16_model.add(vgg16_base)\n",
        "\n",
        "# Flatten and add a Dense layer for classification\n",
        "vgg16_model.add(Flatten())\n",
        "vgg16_model.add(Dense(256, activation='relu'))\n",
        "vgg16_model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "vgg16_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "cZ97ZlH7VROE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train the VGG16 Model\n",
        "# Train the VGG16 model\n",
        "vgg16_model.fit(train_generator,\n",
        "                validation_data=valid_generator,\n",
        "                epochs=1,\n",
        "                steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "                validation_steps=valid_generator.samples // valid_generator.batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPHDM8B9VdVD",
        "outputId": "b483612f-250e-4078-b60d-1fbe7d855aa6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.7458 - loss: 29.1825 - val_accuracy: 0.9866 - val_loss: 0.0641\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78c665128ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NB: the long runtime issue was I was having was due to google colabs on CPU once i changed to Gpu the runtime droped from 5 hours to 5 minutes\n",
        " # Part 2\n",
        "# In this part, you will evaluate your deep learning models on a test data. For this part, you will need to do the following:\n",
        "# 1. Load your saved model that was built using the ResNet50 model.\n",
        "# 2. Construct an ImageDataGenerator for the test set. For this ImageDataGenerator instance, you only need to pass the directory of the test images, target size, and the **shuffle** parameter and set it to False.\n",
        "# 3. Use the **evaluate_generator** method to evaluate your models on the test data, by passing the above ImageDataGenerator as an argument. You can learn more about **evaluate_generator** [here](https://keras.io/models/sequential/).\n",
        "# 4. Print the performance of the classifier using the VGG16 pre-trained model.\n",
        "# 5. Print the performance of the classifier using the ResNet pre-trained model."
      ],
      "metadata": {
        "id": "swKF4L0MW91Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Build the ResNet50-based Classifier\n",
        "# To compare, let’s load and compile a ResNet50-based classifier.\n",
        "# Load ResNet50 without the top layers\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Build a Sequential model\n",
        "resnet50_model = Sequential()\n",
        "\n",
        "# Add ResNet50 base model\n",
        "resnet50_model.add(resnet50_base)\n",
        "\n",
        "# Flatten and add a Dense layer for classification\n",
        "resnet50_model.add(Flatten())\n",
        "resnet50_model.add(Dense(256, activation='relu'))\n",
        "resnet50_model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "resnet50_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "giQMyxyVVqlS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Evaluate the Models on the Test Data\n",
        "# Path to your test dataset\n",
        "test_dir = '/content/concrete_data_week4/test'\n",
        "\n",
        "# Create an ImageDataGenerator for the test set (no data augmentation, just preprocessing)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg)\n",
        "\n",
        "# Load the test data\n",
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                  target_size=(224, 224),\n",
        "                                                  batch_size=100,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=False)\n",
        "\n",
        "# Evaluate the VGG16 model\n",
        "# Workaround to mimic evaluate_generator (if required by the project):\n",
        "vgg16_performance = vgg16_model.evaluate(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(f\"VGG16 Test Accuracy: {vgg16_performance[1]}\")\n",
        "\n",
        "resnet50_performance = resnet50_model.evaluate(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(f\"ResNet50 Test Accuracy: {resnet50_performance[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq8cOyOcVxkl",
        "outputId": "67dc1bf9-8f21-4916-e4db-7a3ec49556c1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 500 images belonging to 2 classes.\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 413ms/step - accuracy: 0.9884 - loss: 0.0297\n",
            "VGG16 Test Accuracy: 0.9879999756813049\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 384ms/step - accuracy: 0.7733 - loss: 0.6364\n",
            "ResNet50 Test Accuracy: 0.6439999938011169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just putting it in a better layout:\n",
        "print(f\"VGG16: {(vgg16_performance[1]*100)} % Test Accuracy\")\n",
        "print(f\"ResNet50: {(resnet50_performance[1]*100)} % Test Accuracy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scU3LWEVXbuK",
        "outputId": "d7cce533-ac3d-4bf0-a0c0-43f414669292"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG16: 98.7999975681305 % Test Accuracy\n",
            "ResNet50: 64.3999993801117 % Test Accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3\n",
        "# In this model, you will predict whether the images in the test data are images of cracked concrete or not. You will do the following:\n",
        "\n",
        "# 1. Use the **predict_generator** method to predict the class of the images in the test data,\n",
        "# by passing the test data ImageDataGenerator instance defined in the previous part as an argument.\n",
        "# You can learn more about the **predict_generator** method [here](https://keras.io/models/sequential/).\n",
        "# 2. Report the class predictions of the first five images in the test set. You should print something list this:\n",
        "# Positive\n",
        "# Negative\n",
        "# Positive\n",
        "# Positive\n",
        "# Negative"
      ],
      "metadata": {
        "id": "kHXE9PtPXg4E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "predictions = vgg16_model.predict_generator(test_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "sABcnyg6X1Ow",
        "outputId": "d99e6f19-bdfc-48a6-c36e-1cc4aad07ba0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Sequential' object has no attribute 'predict_generator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-f5155322dec6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make predictions on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg16_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_generator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using a newer version of Keras, the predict_generator method has been deprecated and removed.\n",
        "# updated predict method, which works with generators in the same way and achieves the same result.\n",
        "# Make predictions on the test data using the updated 'predict' method\n",
        "predictions = vgg16_model.predict(test_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txsvhqcmX6JE",
        "outputId": "9f68d900-f8f1-47eb-961f-8f51c31ea2f8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 415ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class indices from the test generator\n",
        "class_indices = test_generator.class_indices\n",
        "class_labels = {v: k for k, v in class_indices.items()}  # Reverse the class indices to get label names\n",
        "\n",
        "# Convert predictions into class indices (0 or 1)\n",
        "predicted_classes = predictions.argmax(axis=-1)\n",
        "\n",
        "# Convert class indices into human-readable labels\n",
        "predicted_labels = [class_labels[idx] for idx in predicted_classes]\n",
        "\n",
        "# Print the predictions and probabilities for the first five images in the test set\n",
        "print(\"Predictions and Probabilities for the first five images:\")\n",
        "for i in range(5):\n",
        "    label = predicted_labels[i]\n",
        "    probabilities = predictions[i]\n",
        "    print(f\"Image {i+1}: {label.capitalize()} | Probabilities: Negative: {probabilities[0]:.4f}, Positive: {probabilities[1]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2YHXKKdXvbQ",
        "outputId": "f52b2070-2982-44b9-cc6b-9b8ef9cca082"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions and Probabilities for the first five images:\n",
            "Image 1: Negative | Probabilities: Negative: 1.0000, Positive: 0.0000\n",
            "Image 2: Negative | Probabilities: Negative: 1.0000, Positive: 0.0000\n",
            "Image 3: Negative | Probabilities: Negative: 1.0000, Positive: 0.0000\n",
            "Image 4: Negative | Probabilities: Negative: 1.0000, Positive: 0.0000\n",
            "Image 5: Negative | Probabilities: Negative: 0.9994, Positive: 0.0006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "0JD88D_LlL9y",
        "outputId": "4dca8b3f-2cca-4946-ebfb-e722e105c9cb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m6,422,784\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m514\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,422,784</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m63,413,960\u001b[0m (241.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,413,960</span> (241.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,137,986\u001b[0m (80.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,137,986</span> (80.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m42,275,974\u001b[0m (161.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,275,974</span> (161.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}